{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2c53a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 15.0.0\n",
      "Uninstalling pyarrow-15.0.0:\n",
      "  Successfully uninstalled pyarrow-15.0.0\n",
      "Collecting pyarrow==15.0.0\n",
      "  Using cached pyarrow-15.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /Users/justin/miniconda3_Jun2023/envs/lora-llm/lib/python3.10/site-packages (from pyarrow==15.0.0) (1.26.4)\n",
      "Using cached pyarrow-15.0.0-cp310-cp310-macosx_11_0_arm64.whl (24.2 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-15.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y pyarrow\n",
    "!{sys.executable} -m pip install pyarrow==15.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba21563",
   "metadata": {},
   "source": [
    "## 1. Load Energy Document Corpus\n",
    "Use scraped or sample energy technical documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465ad79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow version: 15.0.0\n",
      "Python executable: /Users/justin/miniconda3_Jun2023/envs/lora-llm/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pyarrow\n",
    "import sys\n",
    "print(\"pyarrow version:\", pyarrow.__version__)\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5315b364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oil production forecasting is the prediction of future oil output from wells using historical and engineering data.',\n",
       " 'Reservoir pressure is a key factor affecting oil production rates.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/energy_text.txt') as f:\n",
    "    docs = [line.strip() for line in f if line.strip()]\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece5b04",
   "metadata": {},
   "source": [
    "## 2. Azure OpenAI Embeddings\n",
    "Use text-embedding-ada-002 for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34aafc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/miniconda3_Jun2023/envs/lora-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from typing import List\n",
    "\n",
    "# Try to import sentence-transformers for local fallback\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _has_hf = True\n",
    "except ImportError:\n",
    "    _has_hf = False\n",
    "\n",
    "# Check for Azure OpenAI credentials\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Returns embedding for text using Azure OpenAI if credentials are set, otherwise uses Hugging Face sentence-transformers.\n",
    "    \"\"\"\n",
    "    if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_KEY:\n",
    "        openai.api_type = 'azure'\n",
    "        openai.api_base = AZURE_OPENAI_ENDPOINT\n",
    "        openai.api_key = AZURE_OPENAI_KEY\n",
    "        resp = openai.embeddings.create(input=[text], model='text-embedding-ada-002')\n",
    "        return resp.data[0].embedding\n",
    "    elif _has_hf:\n",
    "        # Use Hugging Face all-MiniLM-L6-v2 as fallback\n",
    "        if not hasattr(get_embedding, '_model'):\n",
    "            get_embedding._model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        return get_embedding._model.encode(text).tolist()\n",
    "    else:\n",
    "        raise RuntimeError('No embedding provider available. Set Azure OpenAI credentials or install sentence-transformers.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead64f47",
   "metadata": {},
   "source": [
    "> **Tip:** This notebook will use Azure OpenAI for embeddings if credentials are set, otherwise it will fall back to Hugging Face sentence-transformers (all-MiniLM-L6-v2). To enable Azure, set `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_KEY` in your environment. To use the local fallback, ensure `sentence-transformers` is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49956955",
   "metadata": {},
   "source": [
    "## 3. Store Embeddings in ChromaDB\n",
    "Vector store for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f0d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection('energy_docs')\n",
    "for i, doc in enumerate(docs):\n",
    "    emb = get_embedding(doc)\n",
    "    collection.add(documents=[doc], embeddings=[emb], ids=[str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ffa78",
   "metadata": {},
   "source": [
    "## 4. RAG Pipeline: Retrieval + Generation\n",
    "Retrieve relevant docs and generate answer with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb62ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m     resp \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      7\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt}]\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m \u001b[43mrag_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHow does reservoir pressure affect oil production?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mrag_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      4\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3_Jun2023/envs/lora-llm/lib/python3.10/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "def rag_query(query):\n",
    "    q_emb = get_embedding(query)\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=3)\n",
    "    context = '\\n'.join([d for d in results['documents'][0]])\n",
    "    prompt = f'Context: {context}\\n\\nQuestion: {query}\\nAnswer:'\n",
    "    # For openai>=1.0.0\n",
    "    response = openai.chat.completions.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "rag_query('How does reservoir pressure affect oil production?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d313269",
   "metadata": {},
   "source": [
    "## 5. Cost Tracking\n",
    "Log token usage and estimate cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: log token usage\n",
    "def log_cost(response):\n",
    "    usage = response['usage']\n",
    "    tokens = usage['total_tokens']\n",
    "    cost = tokens * 0.00002  # Example cost per token\n",
    "    print(f'Tokens: {tokens}, Estimated Cost: ${cost:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8969de",
   "metadata": {},
   "source": [
    "## 6. Prompt Engineering\n",
    "Try few-shot and chain-of-thought prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = '''\n",
    "Context: Oil production depends on reservoir pressure.\n",
    "\n",
    "        Question: What is reservoir pressure?\n",
    "        Answer: Reservoir pressure is the pressure of fluids within a reservoir.\n",
    "        \n",
    "        Question: How does it affect oil production?\n",
    "        Answer: Higher reservoir pressure generally increases oil production.\n",
    "        \n",
    "        Question: What happens when pressure drops?\n",
    "        Answer: Oil production typically declines.\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae545a2a",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "Evaluate retrieval accuracy and answer quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
