{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d966fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Try legacy pip resolver for AzureML install\n",
    "!pip install azureml-core==1.61.0 azureml-widgets==1.61.0 --use-deprecated=legacy-resolver --quiet\n",
    "# If you need notebook features, also run:\n",
    "# !pip install azureml-contrib-notebook==1.61.0 azureml-dataset-runtime==1.61.0 --use-deprecated=legacy-resolver --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d218f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AzureML core packages in smaller groups if needed\n",
    "#!pip install azureml-core==1.61.0 azureml-widgets==1.61.0 --quiet\n",
    "# If you need notebook features, also run:\n",
    "# !pip install azureml-contrib-notebook==1.61.0 azureml-dataset-runtime==1.61.0 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ad7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path for imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0143a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downgrade pip for compatibility, then install correct package versions\n",
    "#!pip install pip==23.3.1 --quiet\n",
    "#!pip install pytorch-forecasting==0.10.3 pytorch-lightning==1.7.7 torch==1.13.1 --quiet\n",
    "# Install compatible torchmetrics version for pytorch-forecasting 0.10.3\n",
    "#!pip install torchmetrics==0.10.0 --quiet\n",
    "# Downgrade numpy for compatibility with pytorch-forecasting 0.10.3\n",
    "#!pip install numpy==1.23.5 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1595ae5",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Generate or load synthetic oil well data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6730c43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>well_id</th>\n",
       "      <th>day</th>\n",
       "      <th>oil_rate</th>\n",
       "      <th>gas_rate</th>\n",
       "      <th>water_cut</th>\n",
       "      <th>choke_size</th>\n",
       "      <th>reservoir_pressure</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1014.901425</td>\n",
       "      <td>491.975591</td>\n",
       "      <td>0.103917</td>\n",
       "      <td>16</td>\n",
       "      <td>3995.964171</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>990.372633</td>\n",
       "      <td>501.742130</td>\n",
       "      <td>0.082081</td>\n",
       "      <td>16</td>\n",
       "      <td>4055.348927</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1008.501804</td>\n",
       "      <td>494.787422</td>\n",
       "      <td>0.111462</td>\n",
       "      <td>16</td>\n",
       "      <td>3916.928967</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1029.342489</td>\n",
       "      <td>493.779319</td>\n",
       "      <td>0.070893</td>\n",
       "      <td>24</td>\n",
       "      <td>4053.685740</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>971.237134</td>\n",
       "      <td>473.670672</td>\n",
       "      <td>0.127177</td>\n",
       "      <td>20</td>\n",
       "      <td>3978.264364</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   well_id  day     oil_rate    gas_rate  water_cut  choke_size  \\\n",
       "0        1    0  1014.901425  491.975591   0.103917          16   \n",
       "1        1    1   990.372633  501.742130   0.082081          16   \n",
       "2        1    2  1008.501804  494.787422   0.111462          16   \n",
       "3        1    3  1029.342489  493.779319   0.070893          24   \n",
       "4        1    4   971.237134  473.670672   0.127177          20   \n",
       "\n",
       "   reservoir_pressure weather  \n",
       "0         3995.964171    rain  \n",
       "1         4055.348927   clear  \n",
       "2         3916.928967   clear  \n",
       "3         4053.685740   storm  \n",
       "4         3978.264364    rain  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data.generate_well_data import generate_synthetic_well_data\n",
    "df = generate_synthetic_well_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b7e88",
   "metadata": {},
   "source": [
    "## 2. Azure ML Experiment Tracking Setup\n",
    "Configure Azure ML workspace and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f6fe200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom azureml.core import Workspace, Experiment\\nws = Workspace.from_config()\\nexperiment = Experiment(ws, 'oil-production-forecasting')\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from azureml.core import Workspace, Experiment\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(ws, 'oil-production-forecasting')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3014ad7",
   "metadata": {},
   "source": [
    "## 3. Model Training: Temporal Fusion Transformer\n",
    "Train TFT on the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40d6beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/miniconda3_Jun2023/envs/energy-ai-azure/lib/python3.10/site-packages/pytorch_lightning/utilities/imports.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.13.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column before drop:\n",
      "well_id               0\n",
      "day                   0\n",
      "oil_rate              0\n",
      "gas_rate              0\n",
      "water_cut             0\n",
      "choke_size            0\n",
      "reservoir_pressure    0\n",
      "weather               0\n",
      "dtype: int64\n",
      "Missing values per column after drop:\n",
      "well_id               0\n",
      "day                   0\n",
      "oil_rate              0\n",
      "gas_rate              0\n",
      "water_cut             0\n",
      "choke_size            0\n",
      "reservoir_pressure    0\n",
      "weather               0\n",
      "dtype: int64\n",
      "Any infinite values: day                   False\n",
      "oil_rate              False\n",
      "gas_rate              False\n",
      "water_cut             False\n",
      "reservoir_pressure    False\n",
      "dtype: bool\n",
      "Group sizes (rows per well_id):\n",
      "count     10.0\n",
      "mean     365.0\n",
      "std        0.0\n",
      "min      365.0\n",
      "25%      365.0\n",
      "50%      365.0\n",
      "75%      365.0\n",
      "max      365.0\n",
      "dtype: float64\n",
      "well_id\n",
      "1     365\n",
      "10    365\n",
      "2     365\n",
      "3     365\n",
      "4     365\n",
      "5     365\n",
      "6     365\n",
      "7     365\n",
      "8     365\n",
      "9     365\n",
      "dtype: int64\n",
      "Number of wells with sufficient history: 10 / 10\n",
      "Shape after filtering short groups: (3650, 8)\n",
      "Batch 0 type: <class 'tuple'>\n",
      "Batch 1 type: <class 'tuple'>\n",
      "Batch 2 type: <class 'tuple'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/miniconda3_Jun2023/envs/energy-ai-azure/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/Users/justin/miniconda3_Jun2023/envs/energy-ai-azure/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/justin/miniconda3_Jun2023/envs/energy-ai-azure/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1789: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/Users/justin/miniconda3_Jun2023/envs/energy-ai-azure/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:107: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 81    \n",
      "3  | prescalers                         | ModuleDict                      | 80    \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 48    \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 3.4 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 1.4 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 119   \n",
      "----------------------------------------------------------------------------------------\n",
      "18.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.7 K    Total params\n",
      "0.075     Total estimated model params size (MB)\n",
      "/Users/justin/miniconda3_Jun2023/envs/energy-ai-azure/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9030b7a3255c433895790d2f11ff1d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/miniconda3_Jun2023/envs/energy-ai-azure/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:471: RuntimeWarning: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "# Ensure well_id and choke_size are string for categorical encoding\n",
    "df[\"well_id\"] = df[\"well_id\"].astype(str)\n",
    "df[\"choke_size\"] = df[\"choke_size\"].astype(str)\n",
    "# Check and drop missing and infinite values\n",
    "print(\"Missing values per column before drop:\")\n",
    "print(df.isnull().sum())\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print(\"Missing values per column after drop:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"Any infinite values:\", (~np.isfinite(df.select_dtypes(include=[float, int]))).any())\n",
    "# --- Filter out groups (wells) with insufficient history for sequence length ---\n",
    "min_encoder_length = 60\n",
    "min_prediction_length = 7\n",
    "min_total_length = min_encoder_length + min_prediction_length\n",
    "group_sizes = df.groupby('well_id').size()\n",
    "print(\"Group sizes (rows per well_id):\")\n",
    "print(group_sizes.describe())\n",
    "print(group_sizes.sort_values())\n",
    "sufficient_history_wells = group_sizes[group_sizes >= min_total_length].index\n",
    "print(f\"Number of wells with sufficient history: {len(sufficient_history_wells)} / {len(group_sizes)}\")\n",
    "df = df[df['well_id'].isin(sufficient_history_wells)].reset_index(drop=True)\n",
    "print(\"Shape after filtering short groups:\", df.shape)\n",
    "# Prepare dataset\n",
    "dataset = TimeSeriesDataSet(\n",
    "    df,\n",
    "    time_idx='day',\n",
    "    target='oil_rate',\n",
    "    group_ids=['well_id'],\n",
    "    min_encoder_length=min_encoder_length,\n",
    "    max_encoder_length=min_encoder_length,\n",
    "    min_prediction_length=min_prediction_length,\n",
    "    max_prediction_length=min_prediction_length,\n",
    "    static_categoricals=['well_id'],\n",
    "    time_varying_known_categoricals=['weather', 'choke_size'],\n",
    "    time_varying_known_reals=['day', 'reservoir_pressure'],\n",
    "    time_varying_unknown_reals=['oil_rate', 'gas_rate', 'water_cut'],\n",
    "    target_normalizer=GroupNormalizer(groups=['well_id']),\n",
    ")\n",
    "# Use to_dataloader() to avoid DataLoader edge cases\n",
    "train_dataloader = dataset.to_dataloader(train=True, batch_size=32, shuffle=True, drop_last=True)\n",
    "# Test DataLoader batches for None\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    if batch is None:\n",
    "        print(f\"Batch {i} is None!\")\n",
    "    else:\n",
    "        print(f\"Batch {i} type: {type(batch)}\")\n",
    "    if i >= 2:\n",
    "        break\n",
    "model = TemporalFusionTransformer.from_dataset(dataset)\n",
    "# Use 'accelerator' and 'devices' instead of deprecated 'gpus' argument\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator='gpu' if torch.cuda.is_available() else 'cpu', devices=1 if torch.cuda.is_available() else 1)\n",
    "trainer.fit(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171ca598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wells with missing days: 0\n"
     ]
    }
   ],
   "source": [
    "# --- Check for missing days in each well_id group ---\n",
    "def check_missing_days(df):\n",
    "    missing = {}\n",
    "    for well, group in df.groupby('well_id'):\n",
    "        expected = set(range(group['day'].min(), group['day'].max() + 1))\n",
    "        actual = set(group['day'])\n",
    "        missing_days = expected - actual\n",
    "        if missing_days:\n",
    "            missing[well] = sorted(missing_days)\n",
    "    return missing\n",
    "\n",
    "missing_days = check_missing_days(df)\n",
    "print(f\"Number of wells with missing days: {len(missing_days)}\")\n",
    "if missing_days:\n",
    "    for well, days in list(missing_days.items())[:3]:\n",
    "        print(f\"well_id {well} missing days: {days[:10]}{'...' if len(days) > 10 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bda0495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column dtypes:\n",
      "well_id                object\n",
      "day                     int64\n",
      "oil_rate              float64\n",
      "gas_rate              float64\n",
      "water_cut             float64\n",
      "choke_size             object\n",
      "reservoir_pressure    float64\n",
      "weather                object\n",
      "dtype: object\n",
      "\n",
      "Sample values for categorical columns:\n",
      "well_id: ['1' '2' '3']\n",
      "choke_size: ['16' '24' '20']\n",
      "weather: ['rain' 'clear' 'storm']\n",
      "\n",
      "First 5 items from TimeSeriesDataSet:\n",
      "Item 0: type=<class 'tuple'>, is None=False\n",
      "Item 1: type=<class 'tuple'>, is None=False\n",
      "Item 2: type=<class 'tuple'>, is None=False\n",
      "Item 3: type=<class 'tuple'>, is None=False\n",
      "Item 4: type=<class 'tuple'>, is None=False\n"
     ]
    }
   ],
   "source": [
    "# --- Check dtypes and sample dataset items ---\n",
    "print(\"Column dtypes:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nSample values for categorical columns:\")\n",
    "print(\"well_id:\", df['well_id'].unique()[:3])\n",
    "print(\"choke_size:\", df['choke_size'].unique()[:3])\n",
    "print(\"weather:\", df['weather'].unique()[:3])\n",
    "\n",
    "print(\"\\nFirst 5 items from TimeSeriesDataSet:\")\n",
    "for i in range(5):\n",
    "    item = dataset[i]\n",
    "    print(f\"Item {i}: type={type(item)}, is None={item is None}\")\n",
    "    if item is None:\n",
    "        print(\"Found None item at index\", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5710ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate (well_id, day) pairs: 0\n",
      "Number of wells with non-monotonic 'day': 0\n",
      "Minimum group size after filtering: 365\n"
     ]
    }
   ],
   "source": [
    "# --- Diagnostics: Check for duplicate (well_id, day) and monotonicity ---\n",
    "duplicates = df.duplicated(subset=[\"well_id\", \"day\"]).sum()\n",
    "print(f\"Number of duplicate (well_id, day) pairs: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(df[df.duplicated(subset=[\"well_id\", \"day\"], keep=False)].sort_values([\"well_id\", \"day\"]))\n",
    "\n",
    "# Check monotonicity of 'day' within each well\n",
    "def is_monotonic(group):\n",
    "    return group[\"day\"].is_monotonic_increasing\n",
    "monotonic = df.groupby(\"well_id\").apply(is_monotonic)\n",
    "print(f\"Number of wells with non-monotonic 'day': {(~monotonic).sum()}\")\n",
    "if (~monotonic).sum() > 0:\n",
    "    print(monotonic[~monotonic])\n",
    "\n",
    "# Print minimum group size after filtering\n",
    "group_sizes = df.groupby('well_id').size()\n",
    "print(f\"Minimum group size after filtering: {group_sizes.min()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c81ca",
   "metadata": {},
   "source": [
    "## 4. Log Experiments to Azure ML\n",
    "Log parameters, metrics, and model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to /Users/justin/energy-ai-azure/notebooks/local_experiment_metrics.json\n",
      "Model checkpoint saved as tft_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Local experiment tracking (no AzureML required)\n",
    "import json\n",
    "import os\n",
    "\n",
    "val_metrics = trainer.callback_metrics if 'trainer' in locals() else {}\n",
    "\n",
    "# Save metrics to a local JSON file\n",
    "metrics_path = os.path.join(os.getcwd(), 'local_experiment_metrics.json')\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'mae': float(val_metrics.get('val_mae', 0)),\n",
    "        'rmse': float(val_metrics.get('val_rmse', 0))\n",
    "    }, f, indent=2)\n",
    "print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Save model weights using PyTorch\n",
    "import torch\n",
    "# Save only the model weights (state_dict)\n",
    "torch.save(model.state_dict(), 'tft_model.pth')\n",
    "print(\"Model weights saved as tft_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb27a0f",
   "metadata": {},
   "source": [
    "## 5. Model Card\n",
    "See [MODEL_CARD.md](../models/production_forecasting/MODEL_CARD.md) for details on intended use, data, evaluation, and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60ae50",
   "metadata": {},
   "source": [
    "## 6. Deploy Model to Azure ML Endpoint\n",
    "Deploy the trained TFT model as a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee94ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See deployment script: models/production_forecasting/deploy_azureml.py\n",
    "# !python ../../models/production_forecasting/deploy_azureml.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fe3d8",
   "metadata": {},
   "source": [
    "## 7. Drift Detection Setup\n",
    "Monitor feature distributions for drift using KS test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe3c8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift in oil_rate: True\n",
      "Drift in gas_rate: True\n",
      "Drift in water_cut: True\n"
     ]
    }
   ],
   "source": [
    "from deployment.drift_detection import detect_drift\n",
    "reference = df[df['day'] < 335]  # First 335 days\n",
    "current = df[df['day'] >= 335]  # Last 30 days\n",
    "for feature in ['oil_rate', 'gas_rate', 'water_cut']:\n",
    "    drift = detect_drift(reference, current, feature)\n",
    "    print(f'Drift in {feature}: {drift}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy-ai-azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
